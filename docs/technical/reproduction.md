# Result Reproduction and Validation

## Overview

Every analysis report generated by this tool includes comprehensive metadata that enables exact reproduction of results. This ensures research integrity, enables collaboration, and provides an audit trail for business-critical analyses.

## Reproducing Results from Reports

All generated markdown reports embed complete reproduction information. Use the dedicated reproduction tool to validate and reproduce results:

```bash
# Reproduce results from a single report
python reproduce_results.py outputs/reports/analysis_20250820_185901.md

# Validate multiple reports with detailed output
python reproduce_results.py --validate --detailed reports/*.md

# Batch validation with custom tolerance
python reproduce_results.py --tolerance 0.02 outputs/reports/

# Summary mode for large report sets
python reproduce_results.py --summary outputs/reports/
```

## Reproduction Features

**Automatic Metadata Extraction**
- Extracts scenario configurations from embedded YAML
- Recovers all resolved parameter values
- Validates checksums and version compatibility

**Intelligent Validation**
- Configurable tolerance levels for numerical comparisons
- Handles floating-point precision differences
- Scenario-specific validation rules
- Confidence scoring and detailed difference reporting

**Comprehensive Reporting**
- Side-by-side comparison of original vs reproduced results
- Root cause analysis for significant differences
- Confidence scores and reproduction quality metrics
- Recommendations for improving reproducibility

## Validation Configuration

Customize validation behavior for specific use cases:

```python
from src.reproducibility.validators import create_validation_config

# Strict validation for critical analyses
strict_config = create_validation_config(tolerance=0.005, strict=True)

# Custom rules for specific metrics
custom_rules = {
    "npv": {"tolerance": 0.02, "type": "percentage"},
    "breakeven_month": {"tolerance": 0, "type": "exact"}
}
config = create_validation_config(custom_rules=custom_rules)
```

## Understanding Results

**Success Indicators:**
- ✅ **High Confidence (95%+)**: All critical metrics match within tolerance
- ⚠️ **Moderate Confidence (80-95%)**: Minor differences, likely due to precision
- ❌ **Low Confidence (<80%)**: Significant differences requiring investigation

**Common Difference Causes:**
- Floating-point precision variations
- Random seed differences (if applicable)
- Dependency version changes
- Model improvements or bug fixes

## Integration with CI/CD

Validate historical reports in automated pipelines:

```bash
# In your CI pipeline
python reproduce_results.py --validate --quiet reports/ || exit 1
```

## Programmatic Reproduction

```python
from src.reproducibility.reproduction_engine import ReproductionEngine

# Initialize reproduction engine
engine = ReproductionEngine()

# Reproduce results from report
result = engine.reproduce_from_report("path/to/report.md", tolerance=0.01)

if result.success:
    print(f"Reproduction successful! Confidence: {result.confidence_score:.1%}")
else:
    print("Reproduction failed:")
    for scenario, differences in result.differences.items():
        print(f"  {scenario}: {differences}")
```

## Reproduction System Guidelines

When contributing to the reproduction system:
1. **Maintain backward compatibility** - existing reports should remain reproducible
2. **Add version information** - include version metadata in new features  
3. **Test thoroughly** - add test cases for new validation rules
4. **Document changes** - update reproduction guidelines for breaking changes
5. **Validate existing reports** - run reproduction tests before committing changes